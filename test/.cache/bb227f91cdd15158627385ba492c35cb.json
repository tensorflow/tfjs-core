{"dependencies":[{"name":"/Users/nsthorat/Code/deeplearnjs-clients/float16/tfjs-core/test/package.json","includedInParent":true,"mtime":1524411533120},{"name":"/Users/nsthorat/Code/deeplearnjs-clients/float16/tfjs-core/test/.babelrc","includedInParent":true,"mtime":1524411533119},{"name":"/Users/nsthorat/Code/deeplearnjs-clients/float16/tfjs-core/tsconfig.json","includedInParent":true,"mtime":1524410852123},{"name":"../doc","loc":{"line":9,"column":20}},{"name":"./adadelta_optimizer","loc":{"line":10,"column":35}},{"name":"./adagrad_optimizer","loc":{"line":11,"column":34}},{"name":"./adam_optimizer","loc":{"line":12,"column":31}},{"name":"./adamax_optimizer","loc":{"line":13,"column":33}},{"name":"./momentum_optimizer","loc":{"line":14,"column":35}},{"name":"./rmsprop_optimizer","loc":{"line":15,"column":34}},{"name":"./sgd_optimizer","loc":{"line":16,"column":30}}],"generated":{"js":"\"use strict\";\nvar __decorate = (this && this.__decorate) || function (decorators, target, key, desc) {\n    var c = arguments.length, r = c < 3 ? target : desc === null ? desc = Object.getOwnPropertyDescriptor(target, key) : desc, d;\n    if (typeof Reflect === \"object\" && typeof Reflect.decorate === \"function\") r = Reflect.decorate(decorators, target, key, desc);\n    else for (var i = decorators.length - 1; i >= 0; i--) if (d = decorators[i]) r = (c < 3 ? d(r) : c > 3 ? d(target, key, r) : d(target, key)) || r;\n    return c > 3 && r && Object.defineProperty(target, key, r), r;\n};\nObject.defineProperty(exports, \"__esModule\", { value: true });\nvar doc_1 = require(\"../doc\");\nvar adadelta_optimizer_1 = require(\"./adadelta_optimizer\");\nvar adagrad_optimizer_1 = require(\"./adagrad_optimizer\");\nvar adam_optimizer_1 = require(\"./adam_optimizer\");\nvar adamax_optimizer_1 = require(\"./adamax_optimizer\");\nvar momentum_optimizer_1 = require(\"./momentum_optimizer\");\nvar rmsprop_optimizer_1 = require(\"./rmsprop_optimizer\");\nvar sgd_optimizer_1 = require(\"./sgd_optimizer\");\nvar OptimizerConstructors = (function () {\n    function OptimizerConstructors() {\n    }\n    OptimizerConstructors.sgd = function (learningRate) {\n        return new sgd_optimizer_1.SGDOptimizer(learningRate);\n    };\n    OptimizerConstructors.momentum = function (learningRate, momentum, useNesterov) {\n        if (useNesterov === void 0) { useNesterov = false; }\n        return new momentum_optimizer_1.MomentumOptimizer(learningRate, momentum, useNesterov);\n    };\n    OptimizerConstructors.rmsprop = function (learningRate, decay, momentum, epsilon, centered) {\n        if (decay === void 0) { decay = .9; }\n        if (momentum === void 0) { momentum = 0.0; }\n        if (epsilon === void 0) { epsilon = 1e-8; }\n        if (centered === void 0) { centered = false; }\n        return new rmsprop_optimizer_1.RMSPropOptimizer(learningRate, decay, momentum, epsilon, centered);\n    };\n    OptimizerConstructors.adam = function (learningRate, beta1, beta2, epsilon) {\n        if (learningRate === void 0) { learningRate = 0.001; }\n        if (beta1 === void 0) { beta1 = 0.9; }\n        if (beta2 === void 0) { beta2 = 0.999; }\n        if (epsilon === void 0) { epsilon = 1e-8; }\n        return new adam_optimizer_1.AdamOptimizer(learningRate, beta1, beta2, epsilon);\n    };\n    OptimizerConstructors.adadelta = function (learningRate, rho, epsilon) {\n        if (learningRate === void 0) { learningRate = .001; }\n        if (rho === void 0) { rho = .95; }\n        if (epsilon === void 0) { epsilon = 1e-8; }\n        return new adadelta_optimizer_1.AdadeltaOptimizer(learningRate, rho, epsilon);\n    };\n    OptimizerConstructors.adamax = function (learningRate, beta1, beta2, epsilon, decay) {\n        if (learningRate === void 0) { learningRate = 0.002; }\n        if (beta1 === void 0) { beta1 = 0.9; }\n        if (beta2 === void 0) { beta2 = 0.999; }\n        if (epsilon === void 0) { epsilon = 1e-8; }\n        if (decay === void 0) { decay = 0.0; }\n        return new adamax_optimizer_1.AdamaxOptimizer(learningRate, beta1, beta2, epsilon, decay);\n    };\n    OptimizerConstructors.adagrad = function (learningRate, initialAccumulatorValue) {\n        if (initialAccumulatorValue === void 0) { initialAccumulatorValue = 0.1; }\n        return new adagrad_optimizer_1.AdagradOptimizer(learningRate, initialAccumulatorValue);\n    };\n    __decorate([\n        doc_1.doc({ heading: 'Training', subheading: 'Optimizers', namespace: 'train' })\n    ], OptimizerConstructors, \"sgd\", null);\n    __decorate([\n        doc_1.doc({ heading: 'Training', subheading: 'Optimizers', namespace: 'train' })\n    ], OptimizerConstructors, \"momentum\", null);\n    __decorate([\n        doc_1.doc({ heading: 'Training', subheading: 'Optimizers', namespace: 'train' })\n    ], OptimizerConstructors, \"rmsprop\", null);\n    __decorate([\n        doc_1.doc({ heading: 'Training', subheading: 'Optimizers', namespace: 'train' })\n    ], OptimizerConstructors, \"adam\", null);\n    __decorate([\n        doc_1.doc({ heading: 'Training', subheading: 'Optimizers', namespace: 'train' })\n    ], OptimizerConstructors, \"adadelta\", null);\n    __decorate([\n        doc_1.doc({ heading: 'Training', subheading: 'Optimizers', namespace: 'train' })\n    ], OptimizerConstructors, \"adamax\", null);\n    __decorate([\n        doc_1.doc({ heading: 'Training', subheading: 'Optimizers', namespace: 'train' })\n    ], OptimizerConstructors, \"adagrad\", null);\n    return OptimizerConstructors;\n}());\nexports.OptimizerConstructors = OptimizerConstructors;\n","map":{"version":3,"file":"optimizer_constructors.js","sourceRoot":"","sources":["../src/optimizers/optimizer_constructors.ts"],"names":[],"mappings":";;;;;;;;AAiBA,8BAA2B;AAE3B,2DAAuD;AACvD,yDAAqD;AACrD,mDAA+C;AAC/C,uDAAmD;AACnD,2DAAuD;AACvD,yDAAqD;AACrD,iDAA6C;AAE7C;IAAA;IA4JA,CAAC;IAvHQ,yBAAG,GAAV,UAAW,YAAoB;QAC7B,OAAO,IAAI,4BAAY,CAAC,YAAY,CAAC,CAAC;IACxC,CAAC;IAgBM,8BAAQ,GAAf,UAAgB,YAAoB,EAAE,QAAgB,EAAE,WAAmB;QAAnB,4BAAA,EAAA,mBAAmB;QAEzE,OAAO,IAAI,sCAAiB,CAAC,YAAY,EAAE,QAAQ,EAAE,WAAW,CAAC,CAAC;IACpE,CAAC;IAqBM,6BAAO,GAAd,UACI,YAAoB,EAAE,KAAU,EAAE,QAAc,EAAE,OAAc,EAChE,QAAgB;QADM,sBAAA,EAAA,UAAU;QAAE,yBAAA,EAAA,cAAc;QAAE,wBAAA,EAAA,cAAc;QAChE,yBAAA,EAAA,gBAAgB;QAElB,OAAO,IAAI,oCAAgB,CAAC,YAAY,EAAE,KAAK,EAAE,QAAQ,EAAE,OAAO,EAChE,QAAQ,CAAC,CAAC;IACd,CAAC;IAaM,0BAAI,GAAX,UAAY,YAAoB,EAAE,KAAW,EAAE,KAAa,EAAE,OAAc;QAAhE,6BAAA,EAAA,oBAAoB;QAAE,sBAAA,EAAA,WAAW;QAAE,sBAAA,EAAA,aAAa;QAAE,wBAAA,EAAA,cAAc;QAE1E,OAAO,IAAI,8BAAa,CAAC,YAAY,EAAE,KAAK,EAAE,KAAK,EAAE,OAAO,CAAC,CAAC;IAChE,CAAC;IAaM,8BAAQ,GAAf,UAAgB,YAAmB,EAAE,GAAS,EAAE,OAAc;QAA9C,6BAAA,EAAA,mBAAmB;QAAE,oBAAA,EAAA,SAAS;QAAE,wBAAA,EAAA,cAAc;QAE5D,OAAO,IAAI,sCAAiB,CAAC,YAAY,EAAE,GAAG,EAAE,OAAO,CAAC,CAAC;IAC3D,CAAC;IAcM,4BAAM,GAAb,UACI,YAAoB,EAAE,KAAW,EAAE,KAAa,EAAE,OAAc,EAChE,KAAW;QADX,6BAAA,EAAA,oBAAoB;QAAE,sBAAA,EAAA,WAAW;QAAE,sBAAA,EAAA,aAAa;QAAE,wBAAA,EAAA,cAAc;QAChE,sBAAA,EAAA,WAAW;QACb,OAAO,IAAI,kCAAe,CAAC,YAAY,EAAE,KAAK,EAAE,KAAK,EAAE,OAAO,EAAE,KAAK,CAAC,CAAC;IACzE,CAAC;IAiBM,6BAAO,GAAd,UAAe,YAAoB,EAAE,uBAA6B;QAA7B,wCAAA,EAAA,6BAA6B;QAEhE,OAAO,IAAI,oCAAgB,CAAC,YAAY,EAAE,uBAAuB,CAAC,CAAC;IACrE,CAAC;IAtHD;QADC,SAAG,CAAC,EAAC,OAAO,EAAE,UAAU,EAAE,UAAU,EAAE,YAAY,EAAE,SAAS,EAAE,OAAO,EAAC,CAAC;0CAGxE;IAgBD;QADC,SAAG,CAAC,EAAC,OAAO,EAAE,UAAU,EAAE,UAAU,EAAE,YAAY,EAAE,SAAS,EAAE,OAAO,EAAC,CAAC;+CAIxE;IAqBD;QADC,SAAG,CAAC,EAAC,OAAO,EAAE,UAAU,EAAE,UAAU,EAAE,YAAY,EAAE,SAAS,EAAE,OAAO,EAAC,CAAC;8CAOxE;IAaD;QADC,SAAG,CAAC,EAAC,OAAO,EAAE,UAAU,EAAE,UAAU,EAAE,YAAY,EAAE,SAAS,EAAE,OAAO,EAAC,CAAC;2CAIxE;IAaD;QADC,SAAG,CAAC,EAAC,OAAO,EAAE,UAAU,EAAE,UAAU,EAAE,YAAY,EAAE,SAAS,EAAE,OAAO,EAAC,CAAC;+CAIxE;IAcD;QADC,SAAG,CAAC,EAAC,OAAO,EAAE,UAAU,EAAE,UAAU,EAAE,YAAY,EAAE,SAAS,EAAE,OAAO,EAAC,CAAC;6CAKxE;IAiBD;QADC,SAAG,CAAC,EAAC,OAAO,EAAE,UAAU,EAAE,UAAU,EAAE,YAAY,EAAE,SAAS,EAAE,OAAO,EAAC,CAAC;8CAIxE;IACH,4BAAC;CAAA,AA5JD,IA4JC;AA5JY,sDAAqB","sourcesContent":["/**\n * @license\n * Copyright 2018 Google Inc. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\n\nimport {doc} from '../doc';\n\nimport {AdadeltaOptimizer} from './adadelta_optimizer';\nimport {AdagradOptimizer} from './adagrad_optimizer';\nimport {AdamOptimizer} from './adam_optimizer';\nimport {AdamaxOptimizer} from './adamax_optimizer';\nimport {MomentumOptimizer} from './momentum_optimizer';\nimport {RMSPropOptimizer} from './rmsprop_optimizer';\nimport {SGDOptimizer} from './sgd_optimizer';\n\nexport class OptimizerConstructors {\n  /**\n   * Constructs a `SGDOptimizer` that uses stochastic gradient descent.\n   *\n   * ```js\n   * // Fit a quadratic function by learning the coefficients a, b, c.\n   * const xs = tf.tensor1d([0, 1, 2, 3]);\n   * const ys = tf.tensor1d([1.1, 5.9, 16.8, 33.9]);\n   *\n   * const a = tf.scalar(Math.random()).variable();\n   * const b = tf.scalar(Math.random()).variable();\n   * const c = tf.scalar(Math.random()).variable();\n   *\n   * // y = a * x^2 + b * x + c.\n   * const f = x => a.mul(x.square()).add(b.mul(x)).add(c);\n   * const loss = (pred, label) => pred.sub(label).square().mean();\n   *\n   * const learningRate = 0.01;\n   * const optimizer = tf.train.sgd(learningRate);\n   *\n   * // Train the model.\n   * for (let i = 0; i < 10; i++) {\n   *   optimizer.minimize(() => loss(f(xs), ys));\n   * }\n   *\n   * // Make predictions.\n   * console.log(\n   *     `a: ${a.dataSync()}, b: ${b.dataSync()}, c: ${c.dataSync()}`);\n   * const preds = f(xs).dataSync();\n   * preds.forEach((pred, i) => {\n   *   console.log(`x: ${i}, pred: ${pred}`);\n   * });\n   * ```\n   *\n   * @param learningRate The learning rate to use for the SGD algorithm.\n   */\n  @doc({heading: 'Training', subheading: 'Optimizers', namespace: 'train'})\n  static sgd(learningRate: number): SGDOptimizer {\n    return new SGDOptimizer(learningRate);\n  }\n\n  /**\n   * Constructs a `MomentumOptimizer` that uses momentum gradient\n   * descent.\n   *\n   * See\n   * [http://proceedings.mlr.press/v28/sutskever13.pdf](\n   * http://proceedings.mlr.press/v28/sutskever13.pdf)\n   *\n   * @param learningRate The learning rate to use for the Momentum gradient\n   * descent algorithm.\n   * @param momentum The momentum to use for the momentum gradient descent\n   * algorithm.\n   */\n  @doc({heading: 'Training', subheading: 'Optimizers', namespace: 'train'})\n  static momentum(learningRate: number, momentum: number, useNesterov = false):\n      MomentumOptimizer {\n    return new MomentumOptimizer(learningRate, momentum, useNesterov);\n  }\n\n  /**\n   * Constructs a `RMSPropOptimizer` that uses RMSProp gradient\n   * descent. This implementation uses plain momentum and is not centered\n   * version of RMSProp.\n   *\n   * See\n   * [http://www.cs.toronto.edu/~tijmen/csc321/slides/lecture_slides_lec6.pdf](\n   * http://www.cs.toronto.edu/~tijmen/csc321/slides/lecture_slides_lec6.pdf)\n   *\n   * @param learningRate The learning rate to use for the RMSProp gradient\n   * descent algorithm.\n   * @param decay The discounting factor for the history/coming gradient.\n   * @param momentum The momentum to use for the RMSProp gradient descent\n   * algorithm.\n   * @param epsilon Small value to avoid zero denominator.\n   * @param centered If true, gradients are normalized by the estimated\n   * variance of the gradient.\n   */\n  @doc({heading: 'Training', subheading: 'Optimizers', namespace: 'train'})\n  static rmsprop(\n      learningRate: number, decay = .9, momentum = 0.0, epsilon = 1e-8,\n      centered = false):\n      RMSPropOptimizer {\n    return new RMSPropOptimizer(learningRate, decay, momentum, epsilon,\n      centered);\n  }\n\n  /**\n   * Constructs a `AdamOptimizer` that uses the Adam algorithm.\n   * See [https://arxiv.org/abs/1412.6980](https://arxiv.org/abs/1412.6980)\n   *\n   * @param learningRate The learning rate to use for the Adam gradient\n   * descent algorithm.\n   * @param beta1 The exponential decay rate for the 1st moment estimates.\n   * @param beta2 The exponential decay rate for the 2nd moment estimates.\n   * @param epsilon A small constant for numerical stability.\n   */\n  @doc({heading: 'Training', subheading: 'Optimizers', namespace: 'train'})\n  static adam(learningRate = 0.001, beta1 = 0.9, beta2 = 0.999, epsilon = 1e-8):\n      AdamOptimizer {\n    return new AdamOptimizer(learningRate, beta1, beta2, epsilon);\n  }\n\n  /**\n   * Constructs a `AdadeltaOptimizer` that uses the Adadelta algorithm.\n   * See [https://arxiv.org/abs/1212.5701](https://arxiv.org/abs/1212.5701)\n   *\n   * @param learningRate The learning rate to use for the Adadelta gradient\n   * descent algorithm.\n   * @param rho The learning rate decay over each update.\n   * @param epsilon A constant epsilon used to better condition the grad\n   * update.\n   */\n  @doc({heading: 'Training', subheading: 'Optimizers', namespace: 'train'})\n  static adadelta(learningRate = .001, rho = .95, epsilon = 1e-8):\n      AdadeltaOptimizer {\n    return new AdadeltaOptimizer(learningRate, rho, epsilon);\n  }\n\n  /**\n   * Constructs a `AdamaxOptimizer` that uses the Adamax algorithm.\n   * See [https://arxiv.org/abs/1412.6980](https://arxiv.org/abs/1412.6980)\n   *\n   * @param learningRate The learning rate to use for the Adamax gradient\n   * descent algorithm.\n   * @param beta1 The exponential decay rate for the 1st moment estimates.\n   * @param beta2 The exponential decay rate for the 2nd moment estimates.\n   * @param epsilon A small constant for numerical stability.\n   * @param decay The learning rate decay over each update.\n   */\n  @doc({heading: 'Training', subheading: 'Optimizers', namespace: 'train'})\n  static adamax(\n      learningRate = 0.002, beta1 = 0.9, beta2 = 0.999, epsilon = 1e-8,\n      decay = 0.0): AdamaxOptimizer {\n    return new AdamaxOptimizer(learningRate, beta1, beta2, epsilon, decay);\n  }\n\n  /**\n   * Constructs a `AdagradOptimizer` that uses the Adagrad algorithm.\n   * See\n   * [http://www.jmlr.org/papers/volume12/duchi11a/duchi11a.pdf](\n   * http://www.jmlr.org/papers/volume12/duchi11a/duchi11a.pdf)\n   * or\n   * [http://ruder.io/optimizing-gradient-descent/index.html#adagrad](\n   * http://ruder.io/optimizing-gradient-descent/index.html#adagrad)\n   *\n   * @param learningRate The learning rate to use for the Adagrad gradient\n   * descent algorithm.\n   * @param initialAccumulatorValue Starting value for the accumulators, must be\n   * positive.\n   */\n  @doc({heading: 'Training', subheading: 'Optimizers', namespace: 'train'})\n  static adagrad(learningRate: number, initialAccumulatorValue = 0.1):\n      AdagradOptimizer {\n    return new AdagradOptimizer(learningRate, initialAccumulatorValue);\n  }\n}\n"]}},"hash":"75c6b4170d37bee7f7c616f38d7b7172","cacheData":{"env":{}}}