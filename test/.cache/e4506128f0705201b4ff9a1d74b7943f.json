{"dependencies":[{"name":"/usr/local/google/home/nsthorat/deeplearnjs-clients/float16real/tfjs-core/test/package.json","includedInParent":true,"mtime":1524156395000},{"name":"/usr/local/google/home/nsthorat/deeplearnjs-clients/float16real/tfjs-core/test/.babelrc","includedInParent":true,"mtime":1524156663000},{"name":"/usr/local/google/home/nsthorat/deeplearnjs-clients/float16real/tfjs-core/tsconfig.json","includedInParent":true,"mtime":1524152197486},{"name":"../environment","loc":{"line":13,"column":28}},{"name":"../globals","loc":{"line":14,"column":24}},{"name":"../ops/ops","loc":{"line":15,"column":20}},{"name":"./optimizer","loc":{"line":16,"column":26}}],"generated":{"js":"\"use strict\";\nvar __extends = (this && this.__extends) || (function () {\n    var extendStatics = Object.setPrototypeOf ||\n        ({ __proto__: [] } instanceof Array && function (d, b) { d.__proto__ = b; }) ||\n        function (d, b) { for (var p in b) if (b.hasOwnProperty(p)) d[p] = b[p]; };\n    return function (d, b) {\n        extendStatics(d, b);\n        function __() { this.constructor = d; }\n        d.prototype = b === null ? Object.create(b) : (__.prototype = b.prototype, new __());\n    };\n})();\nObject.defineProperty(exports, \"__esModule\", { value: true });\nvar environment_1 = require(\"../environment\");\nvar globals_1 = require(\"../globals\");\nvar ops_1 = require(\"../ops/ops\");\nvar optimizer_1 = require(\"./optimizer\");\nvar AdamaxOptimizer = (function (_super) {\n    __extends(AdamaxOptimizer, _super);\n    function AdamaxOptimizer(learningRate, beta1, beta2, epsilon, decay) {\n        if (epsilon === void 0) { epsilon = 1e-8; }\n        if (decay === void 0) { decay = 0.0; }\n        var _this = _super.call(this) || this;\n        _this.learningRate = learningRate;\n        _this.accumulatedFirstMoment = {};\n        _this.accumulatedWeightedInfNorm = {};\n        _this.c = globals_1.keep(ops_1.scalar(-learningRate));\n        _this.eps = globals_1.keep(ops_1.scalar(epsilon));\n        _this.beta1 = globals_1.keep(ops_1.scalar(beta1));\n        _this.beta2 = globals_1.keep(ops_1.scalar(beta2));\n        _this.decay = globals_1.keep(ops_1.scalar(decay));\n        globals_1.tidy(function () {\n            _this.iteration = ops_1.scalar(0).variable();\n            _this.accBeta1 = ops_1.scalar(beta1).variable();\n        });\n        _this.oneMinusBeta1 = globals_1.keep(ops_1.scalar(1 - beta1));\n        _this.one = globals_1.keep(ops_1.scalar(1));\n        return _this;\n    }\n    AdamaxOptimizer.prototype.applyGradients = function (variableGradients) {\n        var _this = this;\n        globals_1.tidy(function () {\n            var oneMinusAccBeta1 = _this.one.sub(_this.accBeta1);\n            var lr = _this.c.div(_this.one.add(_this.decay.mul(_this.iteration)));\n            for (var variableName in variableGradients) {\n                var value = environment_1.ENV.engine.registeredVariables[variableName];\n                if (_this.accumulatedFirstMoment[variableName] == null) {\n                    var trainable = false;\n                    _this.accumulatedFirstMoment[variableName] =\n                        ops_1.zerosLike(value).variable(trainable);\n                }\n                if (_this.accumulatedWeightedInfNorm[variableName] == null) {\n                    var trainable = false;\n                    _this.accumulatedWeightedInfNorm[variableName] =\n                        ops_1.zerosLike(value).variable(trainable);\n                }\n                var gradient = variableGradients[variableName];\n                var firstMoment = _this.accumulatedFirstMoment[variableName];\n                var weightedInfNorm = _this.accumulatedWeightedInfNorm[variableName];\n                var newFirstMoment = _this.beta1.mul(firstMoment).add(_this.oneMinusBeta1.mul(gradient));\n                var ut0 = _this.beta2.mul(weightedInfNorm);\n                var ut1 = gradient.abs();\n                var newWeightedInfNorm = ut0.maximum(ut1);\n                _this.accumulatedFirstMoment[variableName].assign(newFirstMoment);\n                _this.accumulatedWeightedInfNorm[variableName].assign(newWeightedInfNorm);\n                var newValue = lr.div(oneMinusAccBeta1)\n                    .mul(newFirstMoment.div(_this.eps.add(newWeightedInfNorm)))\n                    .add(value);\n                value.assign(newValue);\n            }\n            _this.iteration.assign(_this.iteration.add(_this.one));\n            _this.accBeta1.assign(_this.accBeta1.mul(_this.beta1));\n        });\n    };\n    AdamaxOptimizer.prototype.dispose = function () {\n        var _this = this;\n        this.c.dispose();\n        this.eps.dispose();\n        this.accBeta1.dispose();\n        this.beta1.dispose();\n        this.beta2.dispose();\n        this.oneMinusBeta1.dispose();\n        this.decay.dispose();\n        this.iteration.dispose();\n        this.one.dispose();\n        if (this.accumulatedFirstMoment != null) {\n            Object.keys(this.accumulatedFirstMoment)\n                .forEach(function (name) { return _this.accumulatedFirstMoment[name].dispose(); });\n        }\n        if (this.accumulatedWeightedInfNorm != null) {\n            Object.keys(this.accumulatedWeightedInfNorm)\n                .forEach(function (name) { return _this.accumulatedWeightedInfNorm[name].dispose(); });\n        }\n    };\n    return AdamaxOptimizer;\n}(optimizer_1.Optimizer));\nexports.AdamaxOptimizer = AdamaxOptimizer;\n"},"hash":"798f602297001e1202062abfbb2782b9","cacheData":{"env":{}}}