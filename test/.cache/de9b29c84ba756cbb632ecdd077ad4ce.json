{"dependencies":[{"name":"/usr/local/google/home/nsthorat/deeplearnjs-clients/float16real/tfjs-core/test/package.json","includedInParent":true,"mtime":1524156395000},{"name":"/usr/local/google/home/nsthorat/deeplearnjs-clients/float16real/tfjs-core/test/.babelrc","includedInParent":true,"mtime":1524156663000},{"name":"/usr/local/google/home/nsthorat/deeplearnjs-clients/float16real/tfjs-core/tsconfig.json","includedInParent":true,"mtime":1524152197486},{"name":"./util","loc":{"line":3,"column":19}}],"generated":{"js":"\"use strict\";\nObject.defineProperty(exports, \"__esModule\", { value: true });\nvar util = require(\"./util\");\nfunction getFilteredNodesXToY(tape, xs, y) {\n    var tensorsFromX = {};\n    var nodesFromX = {};\n    for (var i = 0; i < xs.length; i++) {\n        tensorsFromX[xs[i].id] = true;\n    }\n    for (var i = 0; i < tape.length; i++) {\n        var node = tape[i];\n        var nodeInputs = node.inputs;\n        for (var inputName in nodeInputs) {\n            var input = nodeInputs[inputName];\n            var anyInputFromX = false;\n            for (var j = 0; j < xs.length; j++) {\n                if (tensorsFromX[input.id]) {\n                    tensorsFromX[node.output.id] = true;\n                    anyInputFromX = true;\n                    nodesFromX[node.id] = true;\n                    break;\n                }\n            }\n            if (anyInputFromX) {\n                break;\n            }\n        }\n    }\n    var tensorsLeadToY = {};\n    tensorsLeadToY[y.id] = true;\n    var nodesToY = {};\n    for (var i = tape.length - 1; i >= 0; i--) {\n        var node = tape[i];\n        var nodeInputs = node.inputs;\n        var outputs = [];\n        outputs.push(node.output);\n        for (var j = 0; j < outputs.length; j++) {\n            if (tensorsLeadToY[outputs[j].id]) {\n                for (var inputName in nodeInputs) {\n                    tensorsLeadToY[nodeInputs[inputName].id] = true;\n                    nodesToY[node.id] = true;\n                }\n                break;\n            }\n        }\n    }\n    var filteredTape = [];\n    for (var i = 0; i < tape.length; i++) {\n        var node = tape[i];\n        if (nodesFromX[node.id] && nodesToY[node.id]) {\n            var prunedInputs = {};\n            for (var inputName in node.inputs) {\n                var nodeInput = node.inputs[inputName];\n                if (tensorsFromX[nodeInput.id]) {\n                    prunedInputs[inputName] = nodeInput;\n                }\n            }\n            var prunedNode = Object.assign({}, node);\n            prunedNode.inputs = prunedInputs;\n            prunedNode.output = node.output;\n            filteredTape.push(prunedNode);\n        }\n    }\n    return filteredTape;\n}\nexports.getFilteredNodesXToY = getFilteredNodesXToY;\nfunction backpropagateGradients(tensorAccumulatedGradientMap, filteredTape) {\n    for (var i = filteredTape.length - 1; i >= 0; i--) {\n        var node = filteredTape[i];\n        var dy = tensorAccumulatedGradientMap[node.output.id];\n        if (node.gradient == null) {\n            throw new Error(\"Cannot compute gradient: gradient function not found \" +\n                (\"for \" + node.name + \".\"));\n        }\n        var inputGradients = node.gradient(dy);\n        for (var inputName in node.inputs) {\n            if (!(inputName in inputGradients)) {\n                throw new Error(\"Cannot backprop through input \" + inputName + \". \" +\n                    (\"Available gradients found: \" + Object.keys(inputGradients) + \".\"));\n            }\n            var dx = inputGradients[inputName]();\n            var x = node.inputs[inputName];\n            if (!util.arraysEqual(dx.shape, x.shape)) {\n                throw new Error(\"Error in gradient for op \" + node.name + \". The gradient of input \" +\n                    (\"'\" + inputName + \"' has shape '\" + dx.shape + \"', which does not match \") +\n                    (\"the shape of the input '\" + x.shape + \"'\"));\n            }\n            if (tensorAccumulatedGradientMap[x.id] == null) {\n                tensorAccumulatedGradientMap[x.id] = dx;\n            }\n            else {\n                var curGradient = tensorAccumulatedGradientMap[x.id];\n                tensorAccumulatedGradientMap[x.id] = curGradient.add(dx);\n                curGradient.dispose();\n            }\n        }\n    }\n}\nexports.backpropagateGradients = backpropagateGradients;\n"},"hash":"3e33150815c5249309a741e0a395445d","cacheData":{"env":{}}}