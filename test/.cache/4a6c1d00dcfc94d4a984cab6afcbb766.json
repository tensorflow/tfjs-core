{"dependencies":[{"name":"/Users/nsthorat/Code/deeplearnjs-clients/float16/tfjs-core/test/package.json","includedInParent":true,"mtime":1524411533120},{"name":"/Users/nsthorat/Code/deeplearnjs-clients/float16/tfjs-core/test/.babelrc","includedInParent":true,"mtime":1524411533119},{"name":"/Users/nsthorat/Code/deeplearnjs-clients/float16/tfjs-core/tsconfig.json","includedInParent":true,"mtime":1524410852123},{"name":"../doc","loc":{"line":9,"column":20}},{"name":"../globals","loc":{"line":10,"column":24}},{"name":"../util","loc":{"line":11,"column":19}},{"name":"./axis_util","loc":{"line":12,"column":24}},{"name":"./operation","loc":{"line":13,"column":26}},{"name":"./ops","loc":{"line":14,"column":18}}],"generated":{"js":"\"use strict\";\nvar __decorate = (this && this.__decorate) || function (decorators, target, key, desc) {\n    var c = arguments.length, r = c < 3 ? target : desc === null ? desc = Object.getOwnPropertyDescriptor(target, key) : desc, d;\n    if (typeof Reflect === \"object\" && typeof Reflect.decorate === \"function\") r = Reflect.decorate(decorators, target, key, desc);\n    else for (var i = decorators.length - 1; i >= 0; i--) if (d = decorators[i]) r = (c < 3 ? d(r) : c > 3 ? d(target, key, r) : d(target, key)) || r;\n    return c > 3 && r && Object.defineProperty(target, key, r), r;\n};\nObject.defineProperty(exports, \"__esModule\", { value: true });\nvar doc_1 = require(\"../doc\");\nvar globals_1 = require(\"../globals\");\nvar util = require(\"../util\");\nvar axis_util = require(\"./axis_util\");\nvar operation_1 = require(\"./operation\");\nvar ops = require(\"./ops\");\nvar SoftmaxOps = (function () {\n    function SoftmaxOps() {\n    }\n    SoftmaxOps.softmax = function (logits, dim) {\n        if (dim === void 0) { dim = -1; }\n        util.assertArgumentsAreTensors({ logits: logits }, 'softmax');\n        if (dim === -1) {\n            dim = logits.rank - 1;\n        }\n        if (dim !== logits.rank - 1) {\n            throw Error('Softmax along a non-last dimension is not yet supported. ' +\n                (\"Logits was rank \" + logits.rank + \" and dim was \" + dim));\n        }\n        var customOp = globals_1.customGrad(function (logits) {\n            var keepDims = true;\n            var lse = logits.logSumExp([dim], keepDims);\n            var logResult = logits.toFloat().sub(lse);\n            var y = logResult.exp();\n            var gradFunc = function (dy) {\n                var dyTimesY = dy.mul(y);\n                var keepDims = true;\n                return dyTimesY.sub(dyTimesY.sum([dim], keepDims).mul(y));\n            };\n            return { value: y, gradFunc: gradFunc };\n        });\n        return customOp(logits);\n    };\n    SoftmaxOps.softmaxCrossEntropy = function (labels, logits, dim) {\n        if (dim === void 0) { dim = -1; }\n        util.assertArgumentsAreTensors({ labels: labels, logits: logits }, 'softmaxCrossEntropy');\n        util.assertShapesMatch(labels.shape, logits.shape, 'Error in softmaxCrossEntropy: ');\n        if (dim === -1) {\n            dim = logits.rank - 1;\n        }\n        if (dim !== logits.rank - 1) {\n            throw Error(\"Softmax cross entropy along a non-last dimension is not yet \" +\n                (\"supported. Labels / logits was rank \" + logits.rank + \" \") +\n                (\"and dim was \" + dim));\n        }\n        var customOp = globals_1.customGrad(function (labels, logits) {\n            var predictedProbs = logits.softmax(dim);\n            var costVector = ops.scalar(1e-5).add(predictedProbs).log().mul(labels).neg();\n            var value = costVector.sum([dim]);\n            var gradFunc = function (dy) {\n                var dyShape = axis_util.expandShapeToKeepDim(dy.shape, [dim]);\n                return [\n                    dy.reshape(dyShape).mul(labels.toFloat().sub(predictedProbs)),\n                    dy.reshape(dyShape).mul(predictedProbs.sub(labels.toFloat())),\n                ];\n            };\n            return { value: value, gradFunc: gradFunc };\n        });\n        return customOp(labels, logits);\n    };\n    __decorate([\n        doc_1.doc({ heading: 'Operations', subheading: 'Normalization' }),\n        operation_1.operation\n    ], SoftmaxOps, \"softmax\", null);\n    __decorate([\n        doc_1.doc({ heading: 'Training', subheading: 'Losses', namespace: 'losses' }),\n        operation_1.operation\n    ], SoftmaxOps, \"softmaxCrossEntropy\", null);\n    return SoftmaxOps;\n}());\nexports.SoftmaxOps = SoftmaxOps;\n","map":{"version":3,"file":"softmax.js","sourceRoot":"","sources":["../src/ops/softmax.ts"],"names":[],"mappings":";;;;;;;;AAiBA,8BAA2B;AAC3B,sCAAsC;AAEtC,8BAAgC;AAEhC,uCAAyC;AACzC,yCAAsC;AACtC,2BAA6B;AAE7B;IAAA;IAkHA,CAAC;IA5FQ,kBAAO,GAAd,UAAiC,MAAS,EAAE,GAAQ;QAAR,oBAAA,EAAA,OAAO,CAAC;QAClD,IAAI,CAAC,yBAAyB,CAAC,EAAC,MAAM,QAAA,EAAC,EAAE,SAAS,CAAC,CAAC;QAEpD,IAAI,GAAG,KAAK,CAAC,CAAC,EAAE;YACd,GAAG,GAAG,MAAM,CAAC,IAAI,GAAG,CAAC,CAAC;SACvB;QACD,IAAI,GAAG,KAAK,MAAM,CAAC,IAAI,GAAG,CAAC,EAAE;YAC3B,MAAM,KAAK,CACP,2DAA2D;iBAC3D,qBAAmB,MAAM,CAAC,IAAI,qBAAgB,GAAK,CAAA,CAAC,CAAC;SAC1D;QAED,IAAM,QAAQ,GAAG,oBAAU,CAAC,UAAA,MAAM;YAGhC,IAAM,QAAQ,GAAG,IAAI,CAAC;YACtB,IAAM,GAAG,GAAG,MAAM,CAAC,SAAS,CAAC,CAAC,GAAG,CAAC,EAAE,QAAQ,CAAC,CAAC;YAC9C,IAAM,SAAS,GAAG,MAAM,CAAC,OAAO,EAAE,CAAC,GAAG,CAAC,GAAG,CAAC,CAAC;YAC5C,IAAM,CAAC,GAAG,SAAS,CAAC,GAAG,EAAO,CAAC;YAE/B,IAAM,QAAQ,GAAG,UAAC,EAAK;gBACrB,IAAM,QAAQ,GAAG,EAAE,CAAC,GAAG,CAAC,CAAC,CAAC,CAAC;gBAC3B,IAAM,QAAQ,GAAG,IAAI,CAAC;gBACtB,OAAO,QAAQ,CAAC,GAAG,CAAC,QAAQ,CAAC,GAAG,CAAC,CAAC,GAAG,CAAC,EAAE,QAAQ,CAAC,CAAC,GAAG,CAAC,CAAC,CAAC,CAAC,CAAC;YAC5D,CAAC,CAAC;YAEF,OAAO,EAAC,KAAK,EAAE,CAAC,EAAE,QAAQ,UAAA,EAAC,CAAC;QAC9B,CAAC,CAAC,CAAC;QAEH,OAAO,QAAQ,CAAC,MAAM,CAAC,CAAC;IAC1B,CAAC;IA4BM,8BAAmB,GAA1B,UACI,MAAS,EAAE,MAAS,EAAE,GAAQ;QAAR,oBAAA,EAAA,OAAO,CAAC;QAChC,IAAI,CAAC,yBAAyB,CAAC,EAAC,MAAM,QAAA,EAAE,MAAM,QAAA,EAAC,EAAE,qBAAqB,CAAC,CAAC;QACxE,IAAI,CAAC,iBAAiB,CAClB,MAAM,CAAC,KAAK,EAAE,MAAM,CAAC,KAAK,EAAE,gCAAgC,CAAC,CAAC;QAElE,IAAI,GAAG,KAAK,CAAC,CAAC,EAAE;YACd,GAAG,GAAG,MAAM,CAAC,IAAI,GAAG,CAAC,CAAC;SACvB;QACD,IAAI,GAAG,KAAK,MAAM,CAAC,IAAI,GAAG,CAAC,EAAE;YAC3B,MAAM,KAAK,CACP,8DAA8D;iBAC9D,yCAAuC,MAAM,CAAC,IAAI,MAAG,CAAA;iBACrD,iBAAe,GAAK,CAAA,CAAC,CAAC;SAC3B;QAED,IAAM,QAAQ,GAAG,oBAAU,CAAC,UAAC,MAAM,EAAE,MAAM;YACzC,IAAM,cAAc,GAAG,MAAM,CAAC,OAAO,CAAC,GAAG,CAAC,CAAC;YAC3C,IAAM,UAAU,GACZ,GAAG,CAAC,MAAM,CAAC,IAAI,CAAC,CAAC,GAAG,CAAC,cAAc,CAAC,CAAC,GAAG,EAAE,CAAC,GAAG,CAAC,MAAM,CAAC,CAAC,GAAG,EAAE,CAAC;YACjE,IAAM,KAAK,GAAG,UAAU,CAAC,GAAG,CAAC,CAAC,GAAG,CAAC,CAAM,CAAC;YAEzC,IAAM,QAAQ,GAAG,UAAC,EAAK;gBACrB,IAAM,OAAO,GAAG,SAAS,CAAC,oBAAoB,CAAC,EAAE,CAAC,KAAK,EAAE,CAAC,GAAG,CAAC,CAAC,CAAC;gBAChE,OAAO;oBACL,EAAE,CAAC,OAAO,CAAC,OAAO,CAAC,CAAC,GAAG,CAAC,MAAM,CAAC,OAAO,EAAE,CAAC,GAAG,CAAC,cAAc,CAAC,CAAC;oBAC7D,EAAE,CAAC,OAAO,CAAC,OAAO,CAAC,CAAC,GAAG,CAAC,cAAc,CAAC,GAAG,CAAC,MAAM,CAAC,OAAO,EAAE,CAAC,CAAC;iBAC9D,CAAC;YACJ,CAAC,CAAC;YACF,OAAO,EAAC,KAAK,OAAA,EAAE,QAAQ,UAAA,EAAC,CAAC;QAC3B,CAAC,CAAC,CAAC;QAEH,OAAO,QAAQ,CAAC,MAAM,EAAE,MAAM,CAAC,CAAC;IAClC,CAAC;IA3FD;QAFC,SAAG,CAAC,EAAC,OAAO,EAAE,YAAY,EAAE,UAAU,EAAE,eAAe,EAAC,CAAC;QACzD,qBAAS;mCA+BT;IA4BD;QAFC,SAAG,CAAC,EAAC,OAAO,EAAE,UAAU,EAAE,UAAU,EAAE,QAAQ,EAAE,SAAS,EAAE,QAAQ,EAAC,CAAC;QACrE,qBAAS;+CAkCT;IACH,iBAAC;CAAA,AAlHD,IAkHC;AAlHY,gCAAU","sourcesContent":["/**\n * @license\n * Copyright 2018 Google Inc. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\n\nimport {doc} from '../doc';\nimport {customGrad} from '../globals';\nimport {Tensor} from '../tensor';\nimport * as util from '../util';\n\nimport * as axis_util from './axis_util';\nimport {operation} from './operation';\nimport * as ops from './ops';\n\nexport class SoftmaxOps {\n  /**\n   * Computes the softmax normalized vector given the logits.\n   *\n   * ```js\n   * const a = tf.tensor1d([1, 2, 3]);\n   *\n   * a.softmax().print();  // or tf.softmax(a)\n   * ```\n   *\n   * ```js\n   * const a = tf.tensor2d([2, 4, 6, 1, 2, 3], [2, 3]);\n   *\n   * a.softmax().print();  // or tf.softmax(a)\n   * ```\n   *\n   * @param logits The logits array.\n   * @param dim The dimension softmax would be performed on. Defaults to `-1`\n   *     which indicates the last dimension.\n   */\n  @doc({heading: 'Operations', subheading: 'Normalization'})\n  @operation\n  static softmax<T extends Tensor>(logits: T, dim = -1): T {\n    util.assertArgumentsAreTensors({logits}, 'softmax');\n\n    if (dim === -1) {\n      dim = logits.rank - 1;\n    }\n    if (dim !== logits.rank - 1) {\n      throw Error(\n          'Softmax along a non-last dimension is not yet supported. ' +\n          `Logits was rank ${logits.rank} and dim was ${dim}`);\n    }\n\n    const customOp = customGrad(logits => {\n      // Do it in log space for numerical stability.\n      // exp(X - logSumExp(X))\n      const keepDims = true;\n      const lse = logits.logSumExp([dim], keepDims);\n      const logResult = logits.toFloat().sub(lse);\n      const y = logResult.exp() as T;\n\n      const gradFunc = (dy: T) => {\n        const dyTimesY = dy.mul(y);\n        const keepDims = true;\n        return dyTimesY.sub(dyTimesY.sum([dim], keepDims).mul(y));\n      };\n\n      return {value: y, gradFunc};\n    });\n\n    return customOp(logits);\n  }\n\n  /**\n   * Computes softmax cross entropy between logits and labels.\n   *\n   * Measures the probability error in discrete classification tasks in which\n   * the classes are mutually exclusive (each entry is in exactly one class).\n   * For example, each CIFAR-10 image is labeled with one and only one label: an\n   * image can be a dog or a truck, but not both.\n   *\n   * `NOTE`: While the classes are mutually exclusive, their probabilities need\n   * not be. All that is required is that each row of labels is a valid\n   * probability distribution. If they are not, the computation of the gradient\n   * will be incorrect.\n   *\n   * `WARNING`: This op expects unscaled logits, since it performs a softmax on\n   * logits internally for efficiency. Do not call this op with the output of\n   * softmax, as it will produce incorrect results.\n   *\n   * logits and labels must have the same shape, e.g. [batch_size, num_classes]\n   * and the same dtype.\n   * @param labels The labels array.\n   * @param logits The logits array.\n   * @param dim The dimension softmax would be performed on. Defaults to `-1`\n   *     which indicates the last dimension.\n   */\n  @doc({heading: 'Training', subheading: 'Losses', namespace: 'losses'})\n  @operation\n  static softmaxCrossEntropy<T extends Tensor, O extends Tensor>(\n      labels: T, logits: T, dim = -1): O {\n    util.assertArgumentsAreTensors({labels, logits}, 'softmaxCrossEntropy');\n    util.assertShapesMatch(\n        labels.shape, logits.shape, 'Error in softmaxCrossEntropy: ');\n\n    if (dim === -1) {\n      dim = logits.rank - 1;\n    }\n    if (dim !== logits.rank - 1) {\n      throw Error(\n          `Softmax cross entropy along a non-last dimension is not yet ` +\n          `supported. Labels / logits was rank ${logits.rank} ` +\n          `and dim was ${dim}`);\n    }\n    // Use a custom gradient for numerical stability.\n    const customOp = customGrad((labels, logits) => {\n      const predictedProbs = logits.softmax(dim);\n      const costVector =\n          ops.scalar(1e-5).add(predictedProbs).log().mul(labels).neg();\n      const value = costVector.sum([dim]) as O;\n\n      const gradFunc = (dy: O) => {\n        const dyShape = axis_util.expandShapeToKeepDim(dy.shape, [dim]);\n        return [\n          dy.reshape(dyShape).mul(labels.toFloat().sub(predictedProbs)),\n          dy.reshape(dyShape).mul(predictedProbs.sub(labels.toFloat())),\n        ];\n      };\n      return {value, gradFunc};\n    });\n\n    return customOp(labels, logits);\n  }\n}\n"]}},"hash":"4a3e63631f780a5dbfff69084a51affc","cacheData":{"env":{}}}