{"dependencies":[{"name":"/usr/local/google/home/nsthorat/deeplearnjs-clients/float16real/tfjs-core/test/package.json","includedInParent":true,"mtime":1524156395000},{"name":"/usr/local/google/home/nsthorat/deeplearnjs-clients/float16real/tfjs-core/test/.babelrc","includedInParent":true,"mtime":1524156663000},{"name":"/usr/local/google/home/nsthorat/deeplearnjs-clients/float16real/tfjs-core/tsconfig.json","includedInParent":true,"mtime":1524152197486},{"name":"../doc","loc":{"line":9,"column":20}},{"name":"./adadelta_optimizer","loc":{"line":10,"column":35}},{"name":"./adagrad_optimizer","loc":{"line":11,"column":34}},{"name":"./adam_optimizer","loc":{"line":12,"column":31}},{"name":"./adamax_optimizer","loc":{"line":13,"column":33}},{"name":"./momentum_optimizer","loc":{"line":14,"column":35}},{"name":"./rmsprop_optimizer","loc":{"line":15,"column":34}},{"name":"./sgd_optimizer","loc":{"line":16,"column":30}}],"generated":{"js":"\"use strict\";\nvar __decorate = (this && this.__decorate) || function (decorators, target, key, desc) {\n    var c = arguments.length, r = c < 3 ? target : desc === null ? desc = Object.getOwnPropertyDescriptor(target, key) : desc, d;\n    if (typeof Reflect === \"object\" && typeof Reflect.decorate === \"function\") r = Reflect.decorate(decorators, target, key, desc);\n    else for (var i = decorators.length - 1; i >= 0; i--) if (d = decorators[i]) r = (c < 3 ? d(r) : c > 3 ? d(target, key, r) : d(target, key)) || r;\n    return c > 3 && r && Object.defineProperty(target, key, r), r;\n};\nObject.defineProperty(exports, \"__esModule\", { value: true });\nvar doc_1 = require(\"../doc\");\nvar adadelta_optimizer_1 = require(\"./adadelta_optimizer\");\nvar adagrad_optimizer_1 = require(\"./adagrad_optimizer\");\nvar adam_optimizer_1 = require(\"./adam_optimizer\");\nvar adamax_optimizer_1 = require(\"./adamax_optimizer\");\nvar momentum_optimizer_1 = require(\"./momentum_optimizer\");\nvar rmsprop_optimizer_1 = require(\"./rmsprop_optimizer\");\nvar sgd_optimizer_1 = require(\"./sgd_optimizer\");\nvar OptimizerConstructors = (function () {\n    function OptimizerConstructors() {\n    }\n    OptimizerConstructors.sgd = function (learningRate) {\n        return new sgd_optimizer_1.SGDOptimizer(learningRate);\n    };\n    OptimizerConstructors.momentum = function (learningRate, momentum, useNesterov) {\n        if (useNesterov === void 0) { useNesterov = false; }\n        return new momentum_optimizer_1.MomentumOptimizer(learningRate, momentum, useNesterov);\n    };\n    OptimizerConstructors.rmsprop = function (learningRate, decay, momentum, epsilon, centered) {\n        if (decay === void 0) { decay = .9; }\n        if (momentum === void 0) { momentum = 0.0; }\n        if (epsilon === void 0) { epsilon = 1e-8; }\n        if (centered === void 0) { centered = false; }\n        return new rmsprop_optimizer_1.RMSPropOptimizer(learningRate, decay, momentum, epsilon, centered);\n    };\n    OptimizerConstructors.adam = function (learningRate, beta1, beta2, epsilon) {\n        if (learningRate === void 0) { learningRate = 0.001; }\n        if (beta1 === void 0) { beta1 = 0.9; }\n        if (beta2 === void 0) { beta2 = 0.999; }\n        if (epsilon === void 0) { epsilon = 1e-8; }\n        return new adam_optimizer_1.AdamOptimizer(learningRate, beta1, beta2, epsilon);\n    };\n    OptimizerConstructors.adadelta = function (learningRate, rho, epsilon) {\n        if (learningRate === void 0) { learningRate = .001; }\n        if (rho === void 0) { rho = .95; }\n        if (epsilon === void 0) { epsilon = 1e-8; }\n        return new adadelta_optimizer_1.AdadeltaOptimizer(learningRate, rho, epsilon);\n    };\n    OptimizerConstructors.adamax = function (learningRate, beta1, beta2, epsilon, decay) {\n        if (learningRate === void 0) { learningRate = 0.002; }\n        if (beta1 === void 0) { beta1 = 0.9; }\n        if (beta2 === void 0) { beta2 = 0.999; }\n        if (epsilon === void 0) { epsilon = 1e-8; }\n        if (decay === void 0) { decay = 0.0; }\n        return new adamax_optimizer_1.AdamaxOptimizer(learningRate, beta1, beta2, epsilon, decay);\n    };\n    OptimizerConstructors.adagrad = function (learningRate, initialAccumulatorValue) {\n        if (initialAccumulatorValue === void 0) { initialAccumulatorValue = 0.1; }\n        return new adagrad_optimizer_1.AdagradOptimizer(learningRate, initialAccumulatorValue);\n    };\n    __decorate([\n        doc_1.doc({ heading: 'Training', subheading: 'Optimizers', namespace: 'train' })\n    ], OptimizerConstructors, \"sgd\", null);\n    __decorate([\n        doc_1.doc({ heading: 'Training', subheading: 'Optimizers', namespace: 'train' })\n    ], OptimizerConstructors, \"momentum\", null);\n    __decorate([\n        doc_1.doc({ heading: 'Training', subheading: 'Optimizers', namespace: 'train' })\n    ], OptimizerConstructors, \"rmsprop\", null);\n    __decorate([\n        doc_1.doc({ heading: 'Training', subheading: 'Optimizers', namespace: 'train' })\n    ], OptimizerConstructors, \"adam\", null);\n    __decorate([\n        doc_1.doc({ heading: 'Training', subheading: 'Optimizers', namespace: 'train' })\n    ], OptimizerConstructors, \"adadelta\", null);\n    __decorate([\n        doc_1.doc({ heading: 'Training', subheading: 'Optimizers', namespace: 'train' })\n    ], OptimizerConstructors, \"adamax\", null);\n    __decorate([\n        doc_1.doc({ heading: 'Training', subheading: 'Optimizers', namespace: 'train' })\n    ], OptimizerConstructors, \"adagrad\", null);\n    return OptimizerConstructors;\n}());\nexports.OptimizerConstructors = OptimizerConstructors;\n"},"hash":"ee4dfc0177029386f98d693694fe27d2","cacheData":{"env":{}}}